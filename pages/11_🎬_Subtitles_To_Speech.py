import streamlit as st
from openai import OpenAI
import xml.etree.ElementTree as ET
import srt
import re
from datetime import timedelta
import tempfile

from pydub import AudioSegment

from libs.llms import translate_srt, merge_srt
from libs.session import PageSessionState

page_state = PageSessionState("subtitles_to_speech")

page_state.initn_attr("srt_content", "")

# è®¾ç½® Streamlit
st.title("ğŸ¬ å­—å¹•è¯­éŸ³åˆæˆ")

# ä¸Šä¼ å­—å¹•æ–‡ä»¶
uploaded_file = st.file_uploader("ä¸Šä¼ å­—å¹•æ–‡ä»¶", type=["xml", "ttml", "srt"])
col1, col2 = st.columns(2)
parse_btn = col1.button("è§£æå­—å¹•")
clear_btn = col2.button("æ¸…é™¤å­—å¹•")
audio_box = st.container()

with st.sidebar:
    sound_role = st.selectbox("é€‰æ‹©éŸ³è‰²", ["alloy", "echo", "fable", "onyx", "nova", "shimmer"], index=0)
    status_bar = st.progress(0.0, text="")
    ttl_button = st.button("åˆæˆè¯­éŸ³")

if clear_btn:
    page_state.srt_content = ""
    st.rerun()


# å‡½æ•°ï¼šåˆ›å»ºé™éŸ³éŸ³é¢‘
@st.cache_data
def create_silence(duration_milliseconds):
    # ç”ŸæˆæŒ‡å®šæ—¶é•¿çš„é™éŸ³éŸ³é¢‘
    silence = AudioSegment.silent(duration=duration_milliseconds)
    return silence


def merge_overlapping_subtitles(subtitles_src):
    subtitles = [s for s in srt.parse(subtitles_src)]
    merged_subtitles = []
    buffer_sub = subtitles[0]

    for sub in subtitles[1:]:
        if sub.start != buffer_sub.end:
            # åˆå¹¶å­—å¹•
            buffer_sub.content += sub.content
            # buffer_sub.end = max(buffer_sub.end, sub.end)
        else:
            # ä¿å­˜å¹¶å¼€å§‹æ–°çš„å­—å¹•æ®µ
            merged_subtitles.append(buffer_sub)
            buffer_sub = sub

    # æ·»åŠ æœ€åä¸€ä¸ªå­—å¹•æ®µ
    merged_subtitles.append(buffer_sub)
    return srt.compose(merged_subtitles)


def ttml_to_srt(ttml_content):
    # è§£æ TTML å†…å®¹
    root = ET.fromstring(ttml_content)

    # æå– <p> å…ƒç´ ï¼Œè¿™äº›æ˜¯å­—å¹•æ®µ
    subtitles = root.findall('.//{http://www.w3.org/ns/ttml}p')

    # SRT æ ¼å¼çš„ç»“æœ
    srt_result = ""

    for index, subtitle in enumerate(subtitles):
        # æå–å¼€å§‹å’Œç»“æŸæ—¶é—´
        begin = subtitle.get('begin')
        end = subtitle.get('end')

        # è½¬æ¢æ—¶é—´æ ¼å¼ä¸º SRT æ ¼å¼ï¼ˆHH:MM:SS,MMMï¼‰
        begin_srt = re.sub(r'(\d{2}):(\d{2}):(\d{2}).(\d{3})', r'\1:\2:\3,\4', begin)
        end_srt = re.sub(r'(\d{2}):(\d{2}):(\d{2}).(\d{3})', r'\1:\2:\3,\4', end)

        # æå–å­—å¹•æ–‡æœ¬
        text = ''.join(subtitle.itertext())

        # ç»„è£… SRT å­—å¹•æ®µ
        srt_result += f"{index + 1}\n{begin_srt} --> {end_srt}\n{text}\n\n"

    return srt_result


# å‡½æ•°ï¼šè½¬æ¢ XML å­—å¹•åˆ° SRT æ ¼å¼
def xml_to_srt(xml_content):
    root = ET.fromstring(xml_content)
    subtitles = []

    for p in root.findall('.//p'):
        # è·å–å¼€å§‹æ—¶é—´å’ŒæŒç»­æ—¶é—´
        start_time = int(p.get('t', 0))
        duration = int(p.get('d', 0))
        end_time = start_time + duration

        # åˆ›å»º SRT å­—å¹•æ®µ
        subtitle_segment = srt.Subtitle(
            index=len(subtitles) + 1,
            start=timedelta(milliseconds=start_time),
            end=timedelta(milliseconds=end_time),
            content=''.join(s.text for s in p.findall('.//s') if s.text)
        )
        subtitles.append(subtitle_segment)

    return srt.compose(subtitles)


# å‡½æ•°ï¼šç”Ÿæˆè¯­éŸ³æ–‡ä»¶å¹¶è¿”å› AudioSegment å¯¹è±¡
@st.cache_data
def generate_speech_segment(text):
    client = OpenAI()
    response = client.audio.speech.create(
        model="tts-1",
        voice=sound_role,
        input=text
    )
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶è¯»å–ä¸º AudioSegment
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        response.stream_to_file(temp_file.name)
        return AudioSegment.from_mp3(temp_file.name)


# å‡½æ•°ï¼šå¤„ç† SRT å¹¶ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ
def process_srt_and_generate_audio(srt_content):
    segments = []
    previous_end_time = timedelta(0)

    counter = 0
    data = [s for s in srt.parse(srt_content)]
    for subtitle in data:
        if not subtitle.content.strip():
            segments.append(create_silence((subtitle.end - subtitle.start).total_seconds() * 1000))
            continue
        # å¤„ç†å­—å¹•é—´çš„ç©ºç™½æ—¶é—´
        silence_duration_ms = int((subtitle.start - previous_end_time).total_seconds() * 1000)
        if silence_duration_ms > 0:
            segments.append(create_silence(silence_duration_ms))

        # ç”Ÿæˆè¯­éŸ³æ®µ
        speech_segment = generate_speech_segment(subtitle.content)
        segments.append(speech_segment)

        # æ›´æ–°å‰ä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´
        previous_end_time = subtitle.end
        counter += round(1 / len(data), 1)
        status_bar.progress(counter, text=f"åˆå¹¶éŸ³é¢‘æ–‡ä»¶ {counter * 100:.2f}%")

    return segments


# åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
# å‡½æ•°ï¼šåˆå¹¶éŸ³é¢‘ç‰‡æ®µ
def merge_audio_segments(segments):
    combined = AudioSegment.empty()
    for segment in segments:
        combined += segment
    return combined


if parse_btn:
    if uploaded_file is not None:
        file_type = uploaded_file.name.split('.')[-1].lower()
        if file_type == 'xml':
            # è½¬æ¢ XML åˆ° SRT
            page_state.srt_content = xml_to_srt(uploaded_file.getvalue().decode())
        if file_type == 'ttml':
            # è½¬æ¢ XML åˆ° SRT
            page_state.srt_content = ttml_to_srt(uploaded_file.getvalue().decode())
        elif file_type == 'srt':
            # ç›´æ¥è¯»å– SRT æ–‡ä»¶
            page_state.srt_content = uploaded_file.getvalue().decode()
        else:
            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚è¯·ä¸Šä¼  XML æˆ– SRT æ–‡ä»¶ã€‚")
    else:
        st.error("è¯·å…ˆä¸Šä¼ å­—å¹•æ–‡ä»¶ã€‚")

if page_state.srt_content:
    with st.form(key='subtitles_to_speech_form'):
        srt_text = st.text_area("å­—å¹•å†…å®¹ï¼Œå¯ä¿®æ”¹", page_state.srt_content, height=480)
        form_submit_button = st.form_submit_button(label='æ›´æ–°å­—å¹•')
        if form_submit_button:
            ts = list(srt.parse(srt_text))
            for i in range(len(ts)):
                ts[i].index = i + 1
            page_state.srt_content = srt.compose(ts)
            st.rerun()

if ttl_button:
    status_bar.progress(0.0, text=f"å¼€å§‹åˆæˆéŸ³é¢‘æ–‡ä»¶")
    audio_segments = process_srt_and_generate_audio(page_state.srt_content)

    # åˆå¹¶éŸ³é¢‘æ®µ
    merged_audio = merge_audio_segments(audio_segments)
    status_bar.progress(1.0, text=f"åˆå¹¶éŸ³é¢‘æ–‡ä»¶ 100%")

    # å¯¼å‡ºä¸ºæ–‡ä»¶
    merged_audio_path = 'final_audio.mp3'
    merged_audio.export(merged_audio_path, format='mp3')

    # æä¾›ä¸‹è½½é“¾æ¥
    audio_box.audio(merged_audio_path)
    audio_box.download_button(label="ä¸‹è½½éŸ³é¢‘æ–‡ä»¶",
                              data=open(merged_audio_path, 'rb'),
                              file_name="final_audio.mp3",
                              mime="audio/mp3")
