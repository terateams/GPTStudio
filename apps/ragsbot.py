import json
import logging
import streamlit as st
import sys
import os
from dotenv import load_dotenv
from llama_index.chat_engine.types import ChatMode
from llama_index.llms import ChatMessage
from libs.llama_utils import get_llama_memary_index, get_llama_store_index, create_document_index_by_texts, \
    create_document_index_by_files, query_knowledge_data
from libs.session import PageSessionState
from llama_index.tools import QueryEngineTool, ToolMetadata, RetrieverTool
from llama_index.agent import OpenAIAgent
from libs.prompts import get_content_from

#
# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

sys.path.append(os.path.abspath('..'))
load_dotenv()


def get_ragsbot_page(botname):
    page_state = PageSessionState(botname)
    # st.sidebar.markdown("# ğŸ’¡Python ç¼–ç¨‹å¯¼å¸ˆ")

    # ç”¨äºå­˜å‚¨å¯¹è¯è®°å½•, ç¬¬ä¸€æ¡ä¸ºæ¬¢è¿æ¶ˆæ¯
    page_state.initn_attr("messages", [])
    # ç”¨äºæ ‡è®°ä¸Šä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦å·²ç»å¤„ç†
    page_state.initn_attr("last_user_msg_processed", True)
    # ç”¨äºæ ‡è®°æµå¼è¾“å‡ºæ˜¯å¦ç»“æŸ
    page_state.initn_attr("streaming_end", True)

    resp = query_knowledge_data("Radius", "radiusrfc.chroma.db", "radiusrfc")
    if resp:
        for r in resp:
            st.write(r.get_text())
            st.write(r.get_embedding())


    def end_chat_streaming():
        """å½“åœæ­¢æŒ‰é’®è¢«ç‚¹å‡»æ—¶æ‰§è¡Œï¼Œç”¨äºä¿®æ”¹å¤„ç†æ ‡å¿—"""
        page_state.streaming_end = True
        page_state.last_user_msg_processed = True

    def start_chat_streaming():
        """å½“å¼€å§‹æŒ‰é’®è¢«ç‚¹å‡»æ—¶æ‰§è¡Œï¼Œç”¨äºä¿®æ”¹å¤„ç†æ ‡å¿—"""
        page_state.streaming_end = False
        page_state.last_user_msg_processed = False

    def on_input_prompt(iprompt: str):
        if iprompt.strip() == "":
            return
        page_state.chat_prompt = iprompt
        start_chat_streaming()
        page_state.add_chat_msg("messages", {"role": "user", "content": page_state.chat_prompt})
        with st.chat_message("user"):
            st.write(page_state.chat_prompt)

    # æ–‡æœ¬ç±»æ–‡ä»¶ä¸Šä¼ 
    files = st.sidebar.file_uploader("ä¸Šä¼ æ–‡ä»¶", accept_multiple_files=True)

    if st.sidebar.button("ç´¢å¼•æ–‡ä»¶"):
        if files:
            create_document_index_by_files(files, "ragsbot.chroma.db", "default_collection")
            page_state.rags_load = True

    for msg in page_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    if not page_state.rags_load:
        st.warning("è¯·ä¸Šä¼ æ–‡ä»¶")
        st.stop()

    rindex = get_llama_store_index("ragsbot.chroma.db", "default_collection")
    agent = rindex.as_chat_engine(
        chat_mode=ChatMode.CONTEXT,
        context_prompt="""
        "You are a chatbot, able to have normal interactions, as well as talk"
        " about radius tech"
        "Here are the relevant documents for the context:\n"
        "{context_str}"
        "\nInstruction: Use the previous chat history, or the context above, to interact and help the user."
        """,
        verbose=True
    )
    # ç”¨æˆ·è¾“å…¥
    if not page_state.last_user_msg_processed:
        st.chat_input("è¯·ç­‰å¾…ä¸Šä¸€æ¡æ¶ˆæ¯å¤„ç†å®Œæ¯•", disabled=True)
    else:
        if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜"):
            on_input_prompt(prompt)

    stop_action = st.sidebar.empty()
    if not page_state.streaming_end:
        stop_action.button('åœæ­¢è¾“å‡º', on_click=end_chat_streaming, help="ç‚¹å‡»æ­¤æŒ‰é’®åœæ­¢æµå¼è¾“å‡º")

    # ç”¨æˆ·è¾“å…¥å“åº”ï¼Œå¦‚æœä¸Šä¸€æ¡æ¶ˆæ¯ä¸æ˜¯åŠ©æ‰‹çš„æ¶ˆæ¯ï¼Œä¸”ä¸Šä¸€æ¡ç”¨æˆ·æ¶ˆæ¯è¿˜æ²¡æœ‰å¤„ç†å®Œæ¯•
    if (page_state.messages
            and page_state.messages[-1]["role"] != "assistant"
            and not page_state.last_user_msg_processed):
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                messages = [ChatMessage(role=m["role"], content=m["content"]) for m in page_state.messages[-10:-1]]
                response = agent.stream_chat(
                    page_state.chat_prompt,
                    chat_history=messages,
                )
                # æµå¼è¾“å‡º
                placeholder = st.empty()
                full_response = ''
                page_state.add_chat_msg("messages", {"role": "assistant", "content": ""})
                for token in response.response_gen:
                    # # å¦‚æœç”¨æˆ·æ‰‹åŠ¨åœæ­¢äº†æµå¼è¾“å‡ºï¼Œå°±é€€å‡ºå¾ªç¯
                    if page_state.streaming_end:
                        break
                    if token is not None:
                        full_response += token
                        placeholder.markdown(full_response)
                        page_state.update_last_msg("messages", {"role": "assistant", "content": full_response})
                placeholder.markdown(full_response)

        stop_action.empty()
        end_chat_streaming()

    st.sidebar.download_button('å¯¼å‡ºå¯¹è¯å†å²',
                               data=json.dumps(page_state.messages, ensure_ascii=False),
                               file_name="chat_history.json", mime="application/json")

    if st.sidebar.button('æ¸…é™¤å¯¹è¯å†å²'):
        page_state.messages = []
        st.rerun()
